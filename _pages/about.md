---
permalink: /
title: "Aman Singhal"
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

## About Me

I'm a Sr Research Engineer at Together AI, working on making inference faster at scale. Mostly that means speculative decoding—training models, cooking up data recipes, and building out training framework features as needed. Lately I've been digging into long-context training and model distillation.

Before this, I've worked at research labs (OLAB, NYU Langone & CVIT Lab, IIIT-Hyderabad) and industry data science gigs, working on things like information extraction, machine translation, and question-answering.

These days I'm most excited about RL for language models, systems work around attention, and multimodality. Also I keep an eye out for ML infrastructure.

---

## Featured Projects

### ML Algorithms from Scratch
Building core machine learning algorithms from first principles—understanding the math, implementing the code, and exploring the trade-offs. From gradient descent optimizers to neural network architectures, this project digs into what makes ML tick under the hood.

### DPO-RLIF: Reinforcement Learning from Structurally-Derived Implicit Feedback
LLMs in healthcare produce untrustworthy outputs due to biases from human annotators in RLHF and the difficulty of obtaining reliable alignment data at scale. We leverage the structural bias in multiple-choice questions to generate alignment data at medical licensing exam complexity, achieving a 5.5% improvement over SFT on MMLU medical subsets and other clinical evaluation benchmarks.

**Links:** [GitHub Repo](https://github.com/AmanSinghal927/DPO-RL/tree/master) | [Project Report](https://kalpanmukherjee.github.io/dpo-rlif/)
