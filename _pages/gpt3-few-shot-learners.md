---
permalink: /talks/gpt3-few-shot-learners/
title: "GPT-3: Language Models are Few-shot Learners"
excerpt: "Overview of GPT-3 and its impact on large language models"
author_profile: true
---

**Presented by:** Aman Singhal

**Venue:** Large Language and Vision Models Symposium, NYU Center for Data Science

**Year:** 2024

## Abstract

Overview of GPT-3, OpenAI's 175 billion parameter autoregressive language model that achieves strong performance through few-shot learning without task-specific fine-tuning. This talk covers the model's architecture, training data, evaluation results, and limitations, as well as the evolution of subsequent large language models (PaLM, LLaMA, DALL-E, etc.) that built upon GPT-3's foundational insights on scaling.

## Presentation Slides

<div style="position: relative; width: 100%; padding-bottom: 56.25%; margin: 2rem 0;">
  <iframe src="https://docs.google.com/presentation/d/1rzgK9OXseMIdNlFjLvENyz9dvOxUT7Ym/embed?start=false&loop=false&delayms=3000"
          frameborder="0"
          width="100%"
          height="100%"
          allowfullscreen="true"
          mozallowfullscreen="true"
          webkitallowfullscreen="true"
          style="position: absolute; top: 0; left: 0;">
  </iframe>
</div>

[View slides in new window](https://docs.google.com/presentation/d/1rzgK9OXseMIdNlFjLvENyz9dvOxUT7Ym/edit?usp=sharing&ouid=105066931717901017144&rtpof=true&sd=true)

## Key Topics

- GPT-3 architecture and scaling laws
- Few-shot learning without fine-tuning
- Training data and methodology
- Benchmark evaluation results
- Model limitations and challenges
- Evolution of subsequent LLMs (PaLM, LLaMA, DALL-E)
- Impact on the field of natural language processing
