---
permalink: /talks/llama2-open-foundation/
title: "Llama 2: Open Foundation and Fine-Tuned Chat Models"
excerpt: "Meta's open-source LLM with dual reward models for safety and helpfulness"
author_profile: true
---

**Presented by:** Aman Singhal

**Venue:** Large Language and Vision Models Symposium, NYU Center for Data Science

**Year:** 2024

## Abstract

Llama 2 is Meta's open-source LLM trained on 2 trillion tokens, featuring a unique approach of keeping harmful content in pretraining data to teach the model what not to do via RLHF. The model uses two separate reward models—one for helpfulness and one for safety—to balance these sometimes competing objectives. It achieved strong performance on safety benchmarks and spawned over 1,000 research papers within 4 months, enabling work on multimodal AI, adversarial robustness, and various applications.

## Presentation Slides

<div style="position: relative; width: 100%; padding-bottom: 56.25%; margin: 2rem 0;">
  <iframe src="https://docs.google.com/presentation/d/1KszeurKl_O-P8K_YP8tBkGZB6jOl-TPL/embed?start=false&loop=false&delayms=3000"
          frameborder="0"
          width="100%"
          height="100%"
          allowfullscreen="true"
          mozallowfullscreen="true"
          webkitallowfullscreen="true"
          style="position: absolute; top: 0; left: 0;">
  </iframe>
</div>

[View slides in new window](https://docs.google.com/presentation/d/1KszeurKl_O-P8K_YP8tBkGZB6jOl-TPL/edit?usp=sharing&ouid=105066931717901017144&rtpof=true&sd=true)

## Key Topics

- Open-source LLM architecture and training data
- Novel approach to safety: keeping harmful content in pretraining
- Dual reward model system (helpfulness vs. safety)
- RLHF implementation and methodology
- Safety benchmark performance
- Research impact: 1,000+ papers in 4 months
- Applications in multimodal AI and adversarial robustness
- Comparison with other open-source models
